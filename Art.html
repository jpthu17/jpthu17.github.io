<!DOCTYPE html>
<html lang="cx">

<head>
  <title>Xin Chen's Homepage - Shanghaitech University</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8" />
  <meta name="keywords" content="" />
  <script>
    addEventListener("load", function () {
      setTimeout(hideURLbar, 0);
    }, false);

    function hideURLbar() {
      window.scrollTo(0, 1);
    }
  </script>
  <!-- Custom Theme files -->
  <link href="css/bootstrap.css" type="text/css" rel="stylesheet" media="all">
  <link href="css/style.css" type="text/css" rel="stylesheet" media="all">
  <!-- font-awesome icons -->
  <link href="css/fontawesome-all.min.css" rel="stylesheet">
  <!-- //Custom Theme files -->
  <!-- online-fonts -->
  <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900" rel="stylesheet">
  <!-- //online-fonts -->
</head>

<body>
  <!-- <div class="main" id="about"> -->
    <section class="publication" id="publication">
      <div class="container-fluid py-lg-1">
        <h1 class="w3_head mb-5">All Publications - <font size ="5"><a href="./index.html">Back to homepage</a></font></h1>
        <div class="col-lg-12">
        <div class="row paper_box">
        <div class="col-md-2 col-12 paper_img"> 
              <img 
                src="files/Paper/AllPublications/AAAI2022.png" width="90%" playsinline=""
                autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid" />
              </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4>Anisotropic Fourier Features for Neural Image-Based Rendering and Relighting</h4>
              <p>Huangjie Yu, Anpei Chen, <strong>Xin Chen</strong>, <a
                  href="https://www.xu-lan.com/">Lan Xu</a>, Ziyu Shao, <a href="http://www.yu-jingyi.com/">Jingyi Yu</a></p>
              <p>(<strong>AAAI 2022 Oral</strong>) the Association for the Advance of Artificial Intelligence</p>
              <!-- <p class="text-left">We present a fully automatic framework for extracting editable 3D objects directly from a single photograph. </p> -->
              <p>[<a href="./Publication.html">Paper</a>] [BibTex]
              </p>
            </div>
        </div>

        <div class="row paper_box">
        <div class="col-md-2 col-12 paper_img"> 
              <img 
                src="files/Paper/AllPublications/TIM2021.png" width="90%" playsinline=""
                autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid" />
              </div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4>Fusion Images of Versatile Array Sensors for Multi-Object Detection</h4>
              <p>Yinghao Bai, Xinchen Tao, <strong>Xin Chen</strong>, <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>, Chaofeng Ye</p>
              <p>(<strong>TIM 2021</strong>) IEEE Transactions on Instrumentation & Measurement</p>
              <!-- <p class="text-left">We present a fully automatic framework for extracting editable 3D objects directly from a single photograph. </p> -->
              <p>[<a href="./Publication.html">Paper</a>] [BibTex]
              </p>
            </div>
        </div>

          <div class="row paper_box">
            <div class="col-md-2 col-12 paper_img"> <video
                src="files/Paper/TOG2021_TightCap/project_page_TightCap/data/video_teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4><strong>TightCap</strong>: 3D Human Shape Capture with Clothing Tightness Field<br>
              </h4>
              <p><strong>Xin Chen</strong>, Anqi Pang, <a
                  href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en">Wei Yang</a>, Peihao Wang, <a
                  href="https://www.xu-lan.com/">Lan Xu</a>, <a href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p>(<strong>TOG 2021</strong>) ACM Transactions on Graphics</p>
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>[<a href="./TightCap.html">ProjectPage</a>] [<a href="https://arxiv.org/abs/1904.02601">ArxivPage</a>]
                [<a href="files/Paper\TOG2021_TightCap\project_page_TightCap\data\TightCap.pdf" target=_blank>Paper</a>]
                [<a href="files/Paper\TOG2021_TightCap\project_page_TightCap\data\video.mp4" target=_blank>Video</a>]
                [<a href="https://github.com/ChenFengYe/TightCap">Code</a>] [<a
                  href="files\Paper\TOG2021_TightCap\project_page_TightCap\data\bibtex.bib" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-2 col-12 paper_img"> <video
                src="files/Paper/IJCV2020_Sport/project_page_SportsCap/data/video_teaser.mp4" width="90%" playsinline=""
                autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4><strong>SportsCap</strong>: Monocular 3D Human Motion Capture and Fine-grained Understanding in
                Challenging Sports Videos<br>
              </h4>
              <p><strong>Xin Chen</strong>, Anqi Pang, <a
                  href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&hl=en">Wei Yang</a>, <a
                  href="http://yuexinma.me/aboutme.html">Yuexin Ma</a>, <a href="http://xu-lan.com/">Lan Xu</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p>(<strong>IJCV 2021</strong>) International Journal of Computer Vision</p>
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[<a href="./SportsCap.html">ProjectPage</a>] [<a href="https://arxiv.org/abs/2104.11452">ArxivPage</a>]
                [<a href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\SportsCap.pdf">Paper</a>] [<a
                  href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\video.mp4" target=_blank>Video</a>] [<a
                  href="https://github.com/ChenFengYe/SportsCap">Code</a>] [<a
                  href="files\Paper\IJCV2020_Sport\project_page_SportsCap\data\bibtex.bib" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-2 col-12 paper_img"> <video src="files/Paper/CVPR2021_ChallenCap/video_teaser.mp4"
                width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4><strong>ChallenCap</strong>: Monocular 3D Capture of Challenging Human Performances using Multi-Modal
                References<br>
              </h4>
              <p><a href="https://hynann.github.io/">Yannao He</a>, Anqi Pang, <strong>Xin Chen</strong>, Han Liang, <a
                  href="http://yuexinma.me/aboutme.html">Yuexin Ma</a>, <a href="http://xu-lan.com/">Lan Xu</a><br>
              </p>
              <p>(<strong>CVPR 2021 Oral</strong>) Conference on Computer Vision and Pattern Recognition</p>
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[ProjectPage] [<a href="https://arxiv.org/abs/2103.06747">ArxivPage</a>] [<a
                  href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_ChallenCap_Monocular_3D_Capture_of_Challenging_Human_Performances_Using_Multi-Modal_CVPR_2021_paper.pdf"
                  target=_blank>Paper</a>] [<a href="https://www.youtube.com/watch?v=ctF0xUMoN2E"
                  target=_blank>Video</a>] [<a href="files\Paper\CVPR2021_ChallenCap\bibtex.bib"
                  target=_blank>BibTex</a>] </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-2 col-12 paper_img"> <video src="files/Paper/IJCAI2021_FewShot/video_teaser.mp4"
                width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4>Few-shot Neural Human Performance Rendering from Sparse RGBD Videos<br>
              </h4>
              <p>Anqi Pang*, <strong>Xin Chen*</strong>, Haimin Luo, <a href="https://wuminye.com/">Mingye Wu</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="http://xu-lan.com/">Lan Xu</a><br>
              </p>
              <p>(<strong>IJCAI 2021</strong>) International Joint Conference on Artificial Intelligence</p>
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[ProjectPage] [<a href="https://arxiv.org/abs/2107.06505">ArxivPage</a>] [<a
                  href="https://arxiv.org/abs/2107.06505" target=_blank>Paper</a>] [<a href="files\Paper\IJCAI2021_FewShot\video_teaser.mp4"
                  target=_blank>Video</a>] [<a href="files\Paper\IJCAI2021_FewShot\bibtex.bib" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-2 col-12 paper_img"> <video src="files\Paper\MM2021_HOIFVV\video_teaser.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4>Neural Free-Viewpoint Performance Rendering under ComplexHuman-object Interactions<br>
              </h4>
              <p>Guoxing Sun, <strong>Xin Chen</strong>, Yizhang Chen, Anqi Pang, Pei Lin, Yuheng Jiang, <a
                  href="http://xu-lan.com/">Lan Xu</a>, <a
                  href="http://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p>(<strong>ACMMM 2021</strong>) ACM Multimedia</p>
              <!-- <p class="text-left">We propose SportsCap – the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. </p> -->
              <p>[ProjectPage] [<a href="https://arxiv.org/abs/2108.00362">ArxivPage</a>] [<a
                  href="https://arxiv.org/abs/2108.00362" target=_blank>Paper</a>] [<a
                  href="files\Paper\MM2021_HOIFVV\video_teaser.mp4" target=_blank>Video</a>] [<a
                  href="files\Paper\MM2021_HOIFVV\bibtex.bib" target=_blank>BibTex</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-2 col-12 paper_img"> <video src="files/Paper/Arxiv2019_Human/DynamicHuman_teaser.mp4"
                width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4>Multiview Deformation for Dynamic Human Modeling<br>
              </h4>
              <p><a href="http://luoxi.tech/">Xi Luo</a>,<strong>&nbsp;</strong><a href="http://liyuwei.cc/">Yuwei
                  Li</a>,<strong>&nbsp;</strong><strong>Xin Chen</strong>, <a href="http://www.yu-jingyi.com/">Jingyi
                  Yu</a><br>
              </p>
              <p>Arxiv 2019</p>
              <!-- <p class="text-left">We present a novel multi-view dynamic 3D human reconstruction technique based on model-based shape deformation. </p> -->
              <p>[<a href="files/Paper/Arxiv2019_Human/DynamicHuman_tog2020_paper.pdf" target=_blank>Paper</a>] [<a
                  href="files/Paper/Arxiv2019_Human/DynamicHuman_video.mp4" target=_blank>Video</a>] [BibTex] </p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-2 col-12 paper_img"> <video src="files/Paper/ICME2019_Pose/pose2body_teaser.mp4"
                width="90%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4><strong>Pose2Body</strong>: Pose-Guided Human Parts Segmentation</h4>
              <p><strong>Xin Chen</strong>*, <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>*,
                Wangyiteng Zhou, Yingliang Zhang, <a href="http://www.yu-jingyi.com/">Jingyi Yu</a><br>
              </p>
              <p>(<strong>ICME 2019 Oral, 15% accept rate</strong>) IEEE Conference on Multimedia and Expo</p>
              <!-- <p class="text-left">We present a novel technique that we call Pose2Body that robustly conducts human parts segmentation based on the pose estimation results. </p> -->
              <p>[<a href="files/Paper/ICME2019_Pose/Pose2Body_ICME2019_paper.pdf">Paper</a>] [<a
                  href="files/Paper/ICME2019_Pose/Pose2Body_ICME2019_supp.pdf">Supp</a>] [<a
                  href="files/Paper/ICME2019_Pose/Pose2Body.html" target=_blank>BibTex</a>]</p>
            </div>
            <!-- .Service-content ends here -->
          </div>
          <div class="row paper_box">
            <div class="col-md-2 col-12 paper_img"> <video src="files/Paper/CVPR2018_Face/3d_face.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image"
                class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4>Sparse Photometric 3D Face Reconstruction Guided by Morphable Models</h4>
              <p> Xuan Cao, Zhang Chen, Anpei Chen, <strong>Xin Chen</strong>, Shiying Li, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a></p>
              <p>(<strong>CVPR 2018</strong>) IEEE Conference on Computer Vision and Pattern Recognition</p>
              <!-- <p class="text-left">We present a novel 3D face reconstruction technique that leverages sparse photometric stereo (PS) and the latest advances on face registration modeling from a single image. </p> -->
              <p>[<a href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_paper.pdf">Paper</a>] [<a
                  href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_video.mp4">Video</a>] [<a
                  href="files/Paper/CVPR2018_Face/Sparse_Photometric_3D_CVPR_2018_supp.pdf">Supp</a>] [<a
                  href="files/Paper/CVPR2018_Face/cao2018sparse.html" target=_blank>BibTex</a>] </p>
            </div>
          </div>
          <div class="row paper_box">
            <!-- <div class="col-md-2 col-12 paper_img"> <img src="files/Paper/TVCG2018_AutoSweep/AutoSweep.jpg" alt="Popup Image" class="img-fluid" /></div> -->
            <div class="col-md-2 col-12 paper_img"> <video
                src="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/video.mp4" width="90%" playsinline=""
                autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid" /></div>
            <!-- .Icon ends here -->
            <div class="col-md-9 col-12 paper-content">
              <h4><strong>AutoSweep</strong>: Recovering 3D Editable Objects&nbsp;from a Single Photograph</h4>
              <p><strong>Xin Chen</strong>, <a href="http://liyuwei.cc/">Yuwei Li</a>, <a href="http://luoxi.tech/">Xi
                  Luo</a>, <a href="http://tianjiashao.com/">Tianjia Shao</a>, <a
                  href="http://www.yu-jingyi.com/">Jingyi Yu</a>, <a href="http://kunzhou.net/">Kun Zhou</a>, <a
                  href="http://youyizheng.net/">Youyi Zheng </a></p>
              <p>(<strong>TVCG 2018</strong>) IEEE Transactions on Visualization and Computer Graphics</p>
              <!-- <p class="text-left">We present a fully automatic framework for extracting editable 3D objects directly from a single photograph. </p> -->
              <p>[<a href="./AutoSweep.html">ProjectPage</a>] [<a href="https://arxiv.org/abs/2005.13312">ArxivPage</a>]
                [<a href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/AutoSweep.pdf">Paper</a>] [<a
                  href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/video.mp4">Video</a>] [<a
                  href="https://github.com/ChenFengYe/AutoSweep">Code</a>] [<a
                  href="files/Paper/TVCG2018_AutoSweep/project_page_AutoSweep/data/bibtex.bib" target=_blank>BibTex</a>]
              </p>
            </div>
            </div>
    </section>
  <!-- </div> -->
</body>
</html>