<!DOCTYPE html>
<html lang="cx">

<head>
  <title>Peng Jin's Homepage - Peking University</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8" />
  <meta name="keywords" content="" />
  <script>
    addEventListener("load", function () {
      setTimeout(hideURLbar, 0);
    }, false);

    function hideURLbar() {
      window.scrollTo(0, 1);
    }
  </script>
  <!-- Custom Theme files -->
  <link href="css/bootstrap.css" type="text/css" rel="stylesheet" media="all">
  <link href="css/style.css" type="text/css" rel="stylesheet" media="all">
  <!-- font-awesome icons -->
  <link href="css/fontawesome-all.min.css" rel="stylesheet">
  <!-- //Custom Theme files -->
  <!-- online-fonts -->
  <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900" rel="stylesheet">
  <!-- //online-fonts -->
</head>

<body>
  <div class="sidenav">
    <div class="side_top"> <img src="images/001.jpg" alt="news image" class="img-fluid navimg">
      <h1> Peng Jin - 金鹏 <br>
        <!--[CV - <a href="files/CV/chenxin_cv_cn_201803.pdf"><font size="4">中文</font></a> / <a href="files/CV/chenxin_cv_en_202007.pdf">English</a>]<br>-->
        <!--[CV - <font size="4">中文</font>/ <a href="files/CV/chenxin_cv_en_202011.pdf">English</a>]<br>-->
        <a href="files/CV/">CV | Resume</a> - 简历<br>
        <a href="mailto:jp21@stu.pku.edu.cn">Email</a> | <a
          href="https:">LinkedIn</a> | <a
          href="https://github.com/jpthu17">GitHub</a><br>
        <a href="https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM">Conference Timer</a><br>
        <a href="https://scholar.google.com/citations?user=HHXLexAAAAAJ&hl=en">Google Scholar</a>
      </h1>
    </div>
    <!-- header -->
    <header>
      <div class="container-fluid px-5 ">
        <nav class="mnu mx-auto">
          <label for="drop" class="toggle">Menu</label>
          <input type="checkbox" id="drop">
          <ul class="menu">
            <!--<li><a href="index.html">Home</a></li-->
            <li class="mt-sm-0"><a href="#about" class="scroll">Bio</a></li>
            <!-- <li class="mt-sm-3"><a href="./Art.html" class="scroll">Art</a></li> -->
            <li class="mt-sm-3"><a href="#publication" class="scroll">Publications</a></li>
            <!-- <li class="mt-sm-3"><a href="#experience" class="scroll">Experience</a></li> -->
            <!-- <li class="mt-sm-3"><a href="#skill" class="scroll">Skills</a></li> -->
          </ul>
        </nav>
      </div>
    </header>
  </div>
  <div class="main" id="about">
    <div class="banner-text-w3ls">
      <div>
        <div class="mx-auto text-left">
          <h1><strong>Peng Jin - 金鹏</strong></h1>
          <!--<h5>Master student.</h5> -->
          <h6>jp21@stu.pku.edu.cn</h6><br>
          <h5><a href="https://www.pku.edu.cn/"><b>Peking University - 北京大学</b></a></h5>
          <h5><a href="https://www.tsinghua.edu.cn/"><b>Tsinghua University - 清华大学 </b></a></h5>
          <!-- <br> -->
          <!-- <h5><a href="https://www.dgene.com/cn/">DGene</a> </h5> -->
          <!-- <p>. </p> -->
          <!-- <p class="banp mt-5"> </p> -->
          <br>
          I am currently a third-year PhD student in <a href="https://www.pku.edu.cn/">Peking University</a>,
          advised by <a href="https://scholar.google.com/citations?user=-5juAR0AAAAJ&hl=en">Prof. Li Yuan</a>. Before this, I received a
          B.S. degree in <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>.
          <br><br>
          My research interests are <strong>Cross-modal representation learning</strong> and <strong>AI Generated Content</strong>, especially:
          </p>
          <li>Text-Video Retrieval, Cross-Modal Representation Learning</li>
          <li>Multimodal Large Language Models</li>
          <!-- <li>Geometric deep learning</li>        -->
          <!--a class="btn btn-primary mt-lg-5 mt-3 agile-link-bnr" href="#experience" role="button">Learn More</a-->
          <br>
          <font color="#f33"><b>News!</b></font>
          </p>
          <!-- <li>[2022.11]&ensp; Submit four4 Papers.</li> -->
          <li>[2024.02]&ensp; One paper accepted at <strong>CVPR 2024</strong> <font color="#f33"><b>Highlight</b></font>.</li>
          <li>[2023.12]&ensp; One paper accepted at <strong>AAAI 2024</strong>.</li>
          <li>[2023.09]&ensp; One paper accepted at <strong>NeurIPS 2023</strong>.</li>
          <li>[2023.07]&ensp; Two papers accepted at <strong>ICCV 2023</strong>.</li>
          <li>[2023.04]&ensp; One paper accepted at <strong>TIP 2023</strong>.</li>
          <li>[2023.04]&ensp; Three papers accepted at <strong>IJCAI 2023</strong>.</li>
          <li>[2023.02]&ensp; One paper accepted at <strong>CVPR 2023</strong> <font color="#f33"><b>Highlight</b></font>.</li>
          <li>[2022.09]&ensp; One paper accepted at <strong>NeurIPS 2022</strong> <font color="#f33"><b>Spotlight</b></font>.</li>
        </div>
      </div>
    </div>
    <section class="publication" id="publication">
      <div class="container-fluid py-lg-1">
        <h3 class="w3_head mb-5">Selected Publications <font size="4"><a href="https://scholar.google.com/citations?user=HHXLexAAAAAJ&hl=en">(Complete List…)</a>
          </font>
        </h3>
        <!-- <h3 class="w3_head mb-5">Selected Publications (<a href="./Publication.html">Complete List…</a>)</h3> -->
        <div class="row paper_box">
            <div class="col-md-4 col-12 paper_img"> <video
                src="files/Paper/ICCV23_DiffusionRet/Demo.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-8 col-12 paper_content">
              <h5><b>DiffusionRet: Generative Text-Video Retrieval with Diffusion Model<br>
              </b></h5>
              <p><strong>Peng Jin</strong>, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji,
                Chang Liu, Li Yuan, Jie Chen<br>
              </p>
              <p> <font color="#000000"><b>ICCV 2023</b></font></p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>
                [<a href="https://arxiv.org/pdf/2303.09867">Arxiv</a>]
                [<a href="https://github.com/jpthu17/DiffusionRet" target=_blank>Code</a>]
                [<a href="files/Paper/Arxiv23_DiffusionRet/Demo.mp4" target=_blank>Video</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>

        <div class="row paper_box">
            <div class="col-md-4 col-12 paper_img"> <img src="files/Paper/IJCAI23_DiCoSA/1.png" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-8 col-12 paper_content">
              <h5><b>Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment<br>
              </b></h5>
              <p><strong>Peng Jin</strong>, Hao Li, Zesen Cheng, Jinfa Huang, Zhennan Wang, Li Yuan,
                Chang Liu, Jie Chen<br>
              </p>
              <p> <font color="#000000"><b>IJCAI 2023</b></font></p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>
                [<a href="" target=_blank>Paper</a>]
                [<a href="">Arxiv</a>]
                [<a href="https://github.com/jpthu17/DiCoSA" target=_blank>Code</a>]
              </p>
            </div>
          </div>

          <div class="row paper_box">
            <div class="col-md-4 col-12 paper_img"> <img src="files/Paper/IJCAI23_TG-VQA/1.png" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-8 col-12 paper_content">
              <h5><b>TG-VQA: Ternary Game of Video Question Answering<br>
              </b></h5>
              <p>Hao Li*, <strong>Peng Jin*(Equal Contributions)</strong>, Zesen Cheng, Songyang Zhang, Kai Chen, Zhennan Wang,
                Chang Liu, Jie Chen<br>
              </p>
              <p> <font color="#000000"><b>IJCAI 2023</b></font></p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>
                [<a href="" target=_blank>Paper</a>]
                [<a href="">Arxiv</a>]
              </p>
            </div>
          </div>

        <div class="col-lg-12">
          <div class="row paper_box">
            <div class="col-md-4 col-12 paper_img"> <video
                src="files/Paper/CVPR23_HBI/Demo.mp4" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-8 col-12 paper_content">
              <h5><b>Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning<br>
              </b></h5>
              <p><strong>Peng Jin</strong>, JinFa Huang, Pengfei Xiong, Shangxuan Tian, Chang Liu,
                Xiangyang Ji, Li Yuan, Jie Chen<br>
              </p>
              <p> <font color="#000000"><b>CVPR 2023</b></font> <font color="#f33"><b>Highlight (Top 10%)</b></font></p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>
                [<a href="https://jpthu17.github.io/HBI/">Project</a>]
                [<a href="https://arxiv.org/pdf/2303.14369.pdf" target=_blank>Paper</a>]
                [<a href="https://arxiv.org/abs/2303.14369">Arxiv</a>]
                [<a href="https://github.com/jpthu17/HBI" target=_blank>Code</a>]
                [<a href="files/Paper/CVPR23_HBI/Demo.mp4" target=_blank>Video</a>]
              </p>
            </div>
            <!-- .Service-content ends here -->
          </div>

          <div class="row paper_box">
            <div class="col-md-4 col-12 paper_img"> <img src="files/Paper/NeurIPS22_EMCL/1.png" width="90%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
            <!-- .Icon ends here -->
            <div class="col-md-8 col-12 paper_content">
              <h5><b>Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations<br>
              </b></h5>
              <p><strong>Peng Jin</strong>, JinFa Huang, Fenglin Liu, Xian Wu, Shen Ge, Guoli Song,
                David A. Clifton, Jie Chen<br>
              </p>
              <p> <font color="#000000"><b>NeurIPS 2022</b></font> <font color="#f33"><b>Spotlight</b></font></p>
              <!-- <p>(<strong>SIGGRAPH 2022</strong>) ACM Transactions on Graphics, TOG Journal Track</p> -->
              <!-- <p class="text-left">We present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan. </p> -->
              <p>
                [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c355566ce402de341c3320cf69a10750-Paper-Conference.pdf" target=_blank>Paper</a>]
                [<a href="https://arxiv.org/abs/2211.11427">Arxiv</a>]
                [<a href="https://github.com/jpthu17/EMCL" target=_blank>Code</a>]
              </p>
            </div>
          </div>

        </div>
    <br>

    </section>
    <!-- //skill -->
    <!-- <section> -->
    <script type="text/javascript" id="clustrmaps"
            src="//clustrmaps.com/map_v2.js?d=68zbbUPSN_KYpXNHAJBzaPN4ToLTjzDXDRmCfDRJtFw&cl=ffffff&w=a"></script>

    <div>
    <p>
    This cool template is stolen from <a href="https://chenxin.tech/"> Xin Chen</a>!
    </p>
    </div>
</body>

</html>
